{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q5_Q6_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWY4iZW0acJz"
      },
      "source": [
        "# Q5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHi4U1aZ7hh"
      },
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import elementwise_grad\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, hidden_layers , activation_layer, X ,y, flag,alpha):\n",
        "        self.alpha=alpha\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "        self.flag = flag\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.activation_layer = activation_layer\n",
        "        self.W = []\n",
        "        self.B = []\n",
        "        self.W.append(np.ones((hidden_layers[0],X.shape[1])))\n",
        "        for i in range(len(hidden_layers)): \n",
        "            if i==len(hidden_layers)-1:\n",
        "                if flag==0:\n",
        "                    self.W.append(np.ones((1,hidden_layers[-1])))\n",
        "                else:\n",
        "                    self.W.append(np.ones((len(np.unique(y)),hidden_layers[-1])))\n",
        "            else:\n",
        "                self.W.append(np.ones((hidden_layers[i+1],hidden_layers[i])))\n",
        "\n",
        "        for i in range(len(hidden_layers)+1):\n",
        "            if i==len(hidden_layers):\n",
        "                self.B.append(np.zeros(len(np.unique(y))))\n",
        "            else:\n",
        "                self.B.append(np.zeros(hidden_layers[i]))\n",
        "\n",
        "    def grad_func(self,A,y):\n",
        "        A = np.exp(A)\n",
        "        A = A/A.sum(axis = 0)\n",
        "        return -1* np.log(A[y])\n",
        "    \n",
        "    def activation_funtion(self,Z,function):\n",
        "        if function == 'relu':\n",
        "            return np.maximum(0,Z)\n",
        "        elif function == 'sigmoid':\n",
        "            return 1/(1+np.exp(-1*Z))\n",
        "        elif function ==\"identity\":\n",
        "            return Z\n",
        "        else:\n",
        "          print(\"Wrong Activation function\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = np.array(X)\n",
        "        for i in range(len(self.W)):\n",
        "            Z = np.dot(self.W[i],A.T)\n",
        "            for j in range(Z.shape[0]):\n",
        "                for k  in range(Z.shape[1]):\n",
        "                    Z[j,k] = Z[j,k] + self.B[i][j]\n",
        "            A = self.activation_funtion(Z,self.activation_layer[i]).T\n",
        "        if self.flag:\n",
        "            return np.exp(A)/np.exp(A).sum(axis=0)\n",
        "        return A\n",
        "\n",
        "    def backward(self,X,y):\n",
        "        def cost(inp,c):\n",
        "            if c==1:\n",
        "                B=self.B\n",
        "                W=inp\n",
        "            else:\n",
        "                B=inp\n",
        "                W=self.W\n",
        "            x = np.array(X)\n",
        "            for i in range(len(W)):\n",
        "                Z = np.matmul(W[i],x.T) + B[i]\n",
        "                x = self.activation_funtion(Z,self.activation_layer[i]).T\n",
        "            if self.flag:\n",
        "                return self.grad_func(x,y)\n",
        "            return np.square(y-x)\n",
        "\n",
        "        gradient = elementwise_grad(cost)\n",
        "        grad_wrt_w = gradient(self.W,1)\n",
        "        for i in range(len(self.W)):\n",
        "            self.W[i] = self.W[i] - self.alpha * grad_wrt_w[i] \n",
        "        grad_wrt_b = gradient(self.B,0)\n",
        "        for i in range(len(self.B)):\n",
        "            self.B[i] = self.B[i] - self.alpha * grad_wrt_b[i]\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHC3Zej7jCFY"
      },
      "source": [
        "# Q6\n",
        "## Digits Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf112usxNADf",
        "outputId": "37ebfe9d-6cc4-4324-cd86-470411a31a47"
      },
      "source": [
        "def accuracy(y,y_hat):\n",
        "    y_hat_new = []\n",
        "    for i in range(len(y_hat)):\n",
        "        index=0\n",
        "        for j in range(len(y_hat[0])):\n",
        "            if y_hat[i,j]>=y_hat[i,index]:\n",
        "                index = j\n",
        "        y_hat_new.append(index)\n",
        "    return accuracy_score(y,y_hat_new)\n",
        "\n",
        "data = load_digits()\n",
        "X = data.data\n",
        "X = normalize(X)\n",
        "y = data.target\n",
        "avg_accuracy=0\n",
        "layer=[5,6]\n",
        "kf3 = KFold(n_splits=3, shuffle=False)\n",
        "for train_index, test_index in kf3.split(X):\n",
        "    NN = NeuralNetwork(layer, ['sigmoid','relu','sigmoid'], X,y,1,alpha=0.1)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    for i in range(5):\n",
        "        for j in range(X_train.shape[0]):\n",
        "            NN.backward(X_train[j], y_train[j])\n",
        "    y_hat = NN.forward(X_test)\n",
        "    avg_accuracy += accuracy(y_test,y_hat)\n",
        "print(\"Average Accuracy\" ,avg_accuracy/3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Accuracy 0.1001669449081803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI57Jt9MaNAj"
      },
      "source": [
        "## On Bostan Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71V6e6c5vxBY",
        "outputId": "8f0ceab6-c0ad-46fa-efe4-bb71f76ceda7"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "X = normalize(X)\n",
        "layer = [13, 10]\n",
        "rmse = 0\n",
        "kf3 = KFold(n_splits=3, shuffle=False)\n",
        "for train_index, test_index in kf3.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "    NN = NeuralNetwork(layer, ['relu','relu', 'relu'], X,y,0,alpha=0.1)\n",
        "    for i in range(5):\n",
        "        for j in range(X_train.shape[0]):\n",
        "            NN.backward(X_train[j], y_train[j])\n",
        "    y_hat = NN.forward(X_test)\n",
        "    n=X_test.shape[0]\n",
        "    for i in range(n):\n",
        "        rmse += np.sqrt(np.square(y_hat[i][0]-y_test[i])) /n\n",
        "  \n",
        "print(\"Average Rmse value\" ,rmse/3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Rmse value 2.7226659406774427\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}